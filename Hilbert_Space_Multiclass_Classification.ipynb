{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "VqigkNZ1AJr1",
        "outputId": "c65ef5a0-c926-4f97-a271-909db1f18fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe embeddings extracted.\n",
            "Loading IMDB dataset...\n",
            "IMDB dataset loaded and preprocessed.\n",
            "Loading GloVe embeddings...\n",
            "GloVe embeddings loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ complex_embedding (\u001b[38;5;33mComplexEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │      \u001b[38;5;34m17,717,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m101\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ complex_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ComplexEmbedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">17,717,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,717,901\u001b[0m (67.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,717,901</span> (67.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,717,901\u001b[0m (67.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,717,901</span> (67.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 596ms/step - accuracy: 0.5090 - loss: 116.6303 - val_accuracy: 0.7026 - val_loss: 110.2769\n",
            "Epoch 2/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 603ms/step - accuracy: 0.5506 - loss: 108.2666 - val_accuracy: 0.7752 - val_loss: 102.2874\n",
            "Epoch 3/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 568ms/step - accuracy: 0.5961 - loss: 100.4061 - val_accuracy: 0.7682 - val_loss: 94.8436\n",
            "Epoch 4/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 561ms/step - accuracy: 0.6450 - loss: 93.0924 - val_accuracy: 0.7878 - val_loss: 87.9059\n",
            "Epoch 5/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 679ms/step - accuracy: 0.6828 - loss: 86.2745 - val_accuracy: 0.7946 - val_loss: 81.4353\n",
            "Epoch 6/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 626ms/step - accuracy: 0.7115 - loss: 79.9202 - val_accuracy: 0.8068 - val_loss: 75.3931\n",
            "Epoch 7/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 623ms/step - accuracy: 0.7399 - loss: 73.9761 - val_accuracy: 0.8146 - val_loss: 69.7460\n",
            "Epoch 8/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 571ms/step - accuracy: 0.7631 - loss: 68.4241 - val_accuracy: 0.8212 - val_loss: 64.4672\n",
            "Epoch 9/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 601ms/step - accuracy: 0.7718 - loss: 63.2301 - val_accuracy: 0.8334 - val_loss: 59.5280\n",
            "Epoch 10/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 632ms/step - accuracy: 0.7845 - loss: 58.3772 - val_accuracy: 0.8374 - val_loss: 54.9079\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 56ms/step - accuracy: 0.8354 - loss: 54.9121\n",
            "Test Loss: 54.9132\n",
            "Test Accuracy: 0.8349\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Input, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "from tensorflow.keras import regularizers\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# Step 1: Download and Prepare GloVe Embeddings\n",
        "\n",
        "def download_glove_embeddings(glove_dir='glove'):\n",
        "    if not os.path.exists(glove_dir):\n",
        "        os.makedirs(glove_dir)\n",
        "    glove_zip = os.path.join(glove_dir, 'glove.6B.zip')\n",
        "    if not os.path.exists(glove_zip):\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(glove_zip, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    with zipfile.ZipFile(glove_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(glove_dir)\n",
        "    print(\"GloVe embeddings extracted.\")\n",
        "\n",
        "def load_glove_embeddings(glove_file_path, word_index, embedding_dim):\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = ' '.join(values[:-embedding_dim])\n",
        "            coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(\"GloVe embeddings loaded.\")\n",
        "    embedding_matrix = np.random.uniform(-0.05, 0.05, (len(word_index) +1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "# Step 2: Prepare IMDB Dataset\n",
        "\n",
        "def load_imdb_dataset(num_words=20000, maxlen=200):\n",
        "    print(\"Loading IMDB dataset...\")\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
        "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "    index_word = {v:k for k, v in word_index.items()}\n",
        "    # Shift indices by 3 to match Keras's internal processing\n",
        "    word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "    word_index[\"<PAD>\"] = 0\n",
        "    word_index[\"<START>\"] = 1\n",
        "    word_index[\"<UNK>\"] = 2\n",
        "    word_index[\"<UNUSED>\"] = 3\n",
        "    x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
        "    x_test = pad_sequences(x_test, maxlen=maxlen, padding='post')\n",
        "    print(\"IMDB dataset loaded and preprocessed.\")\n",
        "    return (x_train, y_train), (x_test, y_test), word_index\n",
        "\n",
        "# Step 3: Define Custom Complex Embedding Layer\n",
        "\n",
        "class ComplexEmbedding(Layer):\n",
        "    def __init__(self, input_dim, output_dim, embeddings_initializer='uniform',\n",
        "                 embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embeddings_initializer = tf.keras.initializers.get(embeddings_initializer)\n",
        "        self.embeddings_regularizer = tf.keras.regularizers.get(embeddings_regularizer)\n",
        "        self.activity_regularizer = tf.keras.regularizers.get(activity_regularizer)\n",
        "        self.embeddings_constraint = tf.keras.constraints.get(embeddings_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Amplitude (Real part)\n",
        "        self.amplitude_embeddings = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                                    initializer=self.embeddings_initializer,\n",
        "                                                    name='amplitude_embeddings',\n",
        "                                                    regularizer=self.embeddings_regularizer,\n",
        "                                                    constraint=self.embeddings_constraint)\n",
        "        # Phase (Imaginary part)\n",
        "        self.phase_embeddings = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                                initializer=RandomUniform(minval=0, maxval=2*np.pi),\n",
        "                                                name='phase_embeddings',\n",
        "                                                regularizer=self.embeddings_regularizer,\n",
        "                                                constraint=self.embeddings_constraint)\n",
        "        super(ComplexEmbedding, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        amplitude = tf.nn.embedding_lookup(self.amplitude_embeddings, inputs)\n",
        "        phase = tf.nn.embedding_lookup(self.phase_embeddings, inputs)\n",
        "        real_part = amplitude * tf.math.cos(phase)\n",
        "        imag_part = amplitude * tf.math.sin(phase)\n",
        "        complex_embedding = tf.complex(real_part, imag_part)\n",
        "        magnitude = tf.abs(complex_embedding)  # Calculate the magnitude inside the layer\n",
        "        return magnitude\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape + (self.output_dim,)\n",
        "\n",
        "# Step 4: Build and Compile Model\n",
        "\n",
        "def build_model(input_length, vocab_size, embedding_dim, embedding_matrix):\n",
        "    inputs = Input(shape=(input_length,), dtype='int32')\n",
        "    complex_embedding_layer = ComplexEmbedding(input_dim=vocab_size,\n",
        "                                               output_dim=embedding_dim,\n",
        "                                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                               embeddings_regularizer=regularizers.l2(1e-6),\n",
        "                                               name='complex_embedding')\n",
        "    x = complex_embedding_layer(inputs)\n",
        "    x = GlobalAveragePooling1D()(x)  # Use the magnitude of the complex embeddings\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 5: Training and Evaluation\n",
        "\n",
        "def main():\n",
        "    # Parameters\n",
        "    MAX_NUM_WORDS = 20000\n",
        "    MAX_SEQUENCE_LENGTH = 200\n",
        "    EMBEDDING_DIM = 100\n",
        "    GLOVE_DIR = 'glove'\n",
        "    GLOVE_FILE = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
        "\n",
        "    # Download and prepare GloVe embeddings\n",
        "    download_glove_embeddings(GLOVE_DIR)\n",
        "\n",
        "    # Load IMDB dataset\n",
        "    (x_train, y_train), (x_test, y_test), word_index = load_imdb_dataset(num_words=MAX_NUM_WORDS, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    embedding_matrix = load_glove_embeddings(GLOVE_FILE, word_index, EMBEDDING_DIM)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(input_length=MAX_SEQUENCE_LENGTH,\n",
        "                        vocab_size=len(word_index)+1,\n",
        "                        embedding_dim=EMBEDDING_DIM,\n",
        "                        embedding_matrix=embedding_matrix)\n",
        "\n",
        "    # Train model\n",
        "    model.summary()\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=10,\n",
        "              validation_split=0.2)\n",
        "\n",
        "    # Evaluate model\n",
        "    results = model.evaluate(x_test, y_test)\n",
        "    print(f'Test Loss: {results[0]:.4f}')\n",
        "    print(f'Test Accuracy: {results[1]:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Input, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "from tensorflow.keras import regularizers\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# Step 1: Download and Prepare GloVe Embeddings\n",
        "\n",
        "def download_glove_embeddings(glove_dir='glove'):\n",
        "    if not os.path.exists(glove_dir):\n",
        "        os.makedirs(glove_dir)\n",
        "    glove_zip = os.path.join(glove_dir, 'glove.6B.zip')\n",
        "    if not os.path.exists(glove_zip):\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(glove_zip, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    with zipfile.ZipFile(glove_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(glove_dir)\n",
        "    print(\"GloVe embeddings extracted.\")\n",
        "\n",
        "def load_glove_embeddings(glove_file_path, word_index, embedding_dim):\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = ' '.join(values[:-embedding_dim])\n",
        "            coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(\"GloVe embeddings loaded.\")\n",
        "    embedding_matrix = np.random.uniform(-0.05, 0.05, (len(word_index) +1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "# Step 2: Prepare IMDB Dataset\n",
        "\n",
        "def load_imdb_dataset(num_words=20000, maxlen=200):\n",
        "    print(\"Loading IMDB dataset...\")\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
        "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "    index_word = {v:k for k, v in word_index.items()}\n",
        "    # Shift indices by 3 to match Keras's internal processing\n",
        "    word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "    word_index[\"<PAD>\"] = 0\n",
        "    word_index[\"<START>\"] = 1\n",
        "    word_index[\"<UNK>\"] = 2\n",
        "    word_index[\"<UNUSED>\"] = 3\n",
        "    x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
        "    x_test = pad_sequences(x_test, maxlen=maxlen, padding='post')\n",
        "    print(\"IMDB dataset loaded and preprocessed.\")\n",
        "    return (x_train, y_train), (x_test, y_test), word_index\n",
        "\n",
        "# Step 3: Define Custom Complex Embedding Layer\n",
        "\n",
        "class ComplexEmbedding(Layer):\n",
        "    def __init__(self, input_dim, output_dim, embeddings_initializer='uniform',\n",
        "                 embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embeddings_initializer = tf.keras.initializers.get(embeddings_initializer)\n",
        "        self.embeddings_regularizer = tf.keras.regularizers.get(embeddings_regularizer)\n",
        "        self.activity_regularizer = tf.keras.regularizers.get(activity_regularizer)\n",
        "        self.embeddings_constraint = tf.keras.constraints.get(embeddings_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Amplitude (Real part)\n",
        "        self.amplitude_embeddings = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                                    initializer=self.embeddings_initializer,\n",
        "                                                    name='amplitude_embeddings',\n",
        "                                                    regularizer=self.embeddings_regularizer,\n",
        "                                                    constraint=self.embeddings_constraint)\n",
        "        # Phase (Imaginary part)\n",
        "        self.phase_embeddings = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                                initializer=RandomUniform(minval=0, maxval=2*np.pi),\n",
        "                                                name='phase_embeddings',\n",
        "                                                regularizer=self.embeddings_regularizer,\n",
        "                                                constraint=self.embeddings_constraint)\n",
        "        super(ComplexEmbedding, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        amplitude = tf.nn.embedding_lookup(self.amplitude_embeddings, inputs)\n",
        "        phase = tf.nn.embedding_lookup(self.phase_embeddings, inputs)\n",
        "        real_part = amplitude * tf.math.cos(phase)\n",
        "        imag_part = amplitude * tf.math.sin(phase)\n",
        "        complex_embedding = tf.complex(real_part, imag_part)\n",
        "        magnitude = tf.abs(complex_embedding)  # Calculate the magnitude inside the layer\n",
        "        return magnitude\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape + (self.output_dim,)\n",
        "\n",
        "# Step 4: Build and Compile Model\n",
        "\n",
        "def build_model(input_length, vocab_size, embedding_dim, embedding_matrix):\n",
        "    inputs = Input(shape=(input_length,), dtype='int32')\n",
        "    complex_embedding_layer = ComplexEmbedding(input_dim=vocab_size,\n",
        "                                               output_dim=embedding_dim,\n",
        "                                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                               embeddings_regularizer=regularizers.l2(1e-6),\n",
        "                                               name='complex_embedding')\n",
        "    x = complex_embedding_layer(inputs)\n",
        "    x = GlobalAveragePooling1D()(x)  # Use the magnitude of the complex embeddings\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 5: Process and Analyze a Test Sentence\n",
        "\n",
        "def process_sentence(sentence, model, tokenizer, maxlen):\n",
        "    # Tokenize and pad the sentence\n",
        "    print(f\"Original Sentence: {sentence}\")\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    print(f\"Tokenized Sentence: {sequence}\")\n",
        "\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=maxlen, padding='post')\n",
        "    print(f\"Padded Sentence: {padded_sequence}\")\n",
        "\n",
        "    # Extract the complex embedding layer from the model\n",
        "    complex_embedding_layer = model.get_layer('complex_embedding')\n",
        "\n",
        "    # Get the embedding output\n",
        "    embedding_output = complex_embedding_layer(padded_sequence)\n",
        "    print(f\"Complex Embedding Magnitude (Amplitude):\\n{embedding_output.numpy()}\")\n",
        "\n",
        "    # Apply Global Average Pooling\n",
        "    pooled_output = GlobalAveragePooling1D()(embedding_output)\n",
        "    print(f\"Global Average Pooled Output:\\n{pooled_output.numpy()}\")\n",
        "\n",
        "    # Dense layer output\n",
        "    final_output = model.layers[-1](pooled_output)\n",
        "    print(f\"Final Output (after Dense Layer):\\n{final_output.numpy()}\")\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Step 6: Training, Evaluation, and Sentence Processing\n",
        "\n",
        "def main():\n",
        "    # Parameters\n",
        "    MAX_NUM_WORDS = 20000\n",
        "    MAX_SEQUENCE_LENGTH = 200\n",
        "    EMBEDDING_DIM = 100\n",
        "    GLOVE_DIR = 'glove'\n",
        "    GLOVE_FILE = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
        "\n",
        "    # Sample test sentence\n",
        "    test_sentence = \"This subject is called quantum natural language.\"\n",
        "\n",
        "    # Download and prepare GloVe embeddings\n",
        "    download_glove_embeddings(GLOVE_DIR)\n",
        "\n",
        "    # Load IMDB dataset\n",
        "    (x_train, y_train), (x_test, y_test), word_index = load_imdb_dataset(num_words=MAX_NUM_WORDS, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    embedding_matrix = load_glove_embeddings(GLOVE_FILE, word_index, EMBEDDING_DIM)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(input_length=MAX_SEQUENCE_LENGTH,\n",
        "                        vocab_size=len(word_index)+1,\n",
        "                        embedding_dim=EMBEDDING_DIM,\n",
        "                        embedding_matrix=embedding_matrix)\n",
        "\n",
        "    # Train model (optional, you may skip this if you don't need training)\n",
        "    model.summary()\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=10,\n",
        "              validation_split=0.2)\n",
        "\n",
        "    # Evaluate model\n",
        "    results = model.evaluate(x_test, y_test)\n",
        "    print(f'Test Loss: {results[0]:.4f}')\n",
        "    print(f'Test Accuracy: {results[1]:.4f}')\n",
        "\n",
        "    # Tokenizer setup for the test sentence\n",
        "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "    tokenizer.fit_on_texts([test_sentence])\n",
        "\n",
        "    # Process the test sentence\n",
        "    process_sentence(test_sentence, model, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U_ChaE1QilbJ",
        "outputId": "d6a2fbc2-c950-4faa-cc09-3c37cb1a73e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading GloVe embeddings...\n",
            "Download complete.\n",
            "GloVe embeddings extracted.\n",
            "Loading IMDB dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "IMDB dataset loaded and preprocessed.\n",
            "Loading GloVe embeddings...\n",
            "GloVe embeddings loaded.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ complex_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ComplexEmbedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">17,717,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ complex_embedding (\u001b[38;5;33mComplexEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │      \u001b[38;5;34m17,717,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m101\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,717,901</span> (67.59 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,717,901\u001b[0m (67.59 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,717,901</span> (67.59 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,717,901\u001b[0m (67.59 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 564ms/step - accuracy: 0.5085 - loss: 116.6536 - val_accuracy: 0.6778 - val_loss: 110.3025\n",
            "Epoch 2/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 553ms/step - accuracy: 0.5405 - loss: 108.2898 - val_accuracy: 0.7652 - val_loss: 102.3113\n",
            "Epoch 3/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 559ms/step - accuracy: 0.5964 - loss: 100.4271 - val_accuracy: 0.7506 - val_loss: 94.8675\n",
            "Epoch 4/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 518ms/step - accuracy: 0.6405 - loss: 93.1114 - val_accuracy: 0.7870 - val_loss: 87.9300\n",
            "Epoch 5/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 520ms/step - accuracy: 0.6828 - loss: 86.2958 - val_accuracy: 0.7946 - val_loss: 81.4579\n",
            "Epoch 6/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 500ms/step - accuracy: 0.7180 - loss: 79.9324 - val_accuracy: 0.8020 - val_loss: 75.4138\n",
            "Epoch 7/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 504ms/step - accuracy: 0.7313 - loss: 74.0020 - val_accuracy: 0.8150 - val_loss: 69.7661\n",
            "Epoch 8/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 503ms/step - accuracy: 0.7622 - loss: 68.4381 - val_accuracy: 0.8218 - val_loss: 64.4842\n",
            "Epoch 9/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 513ms/step - accuracy: 0.7802 - loss: 63.2450 - val_accuracy: 0.8306 - val_loss: 59.5448\n",
            "Epoch 10/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 500ms/step - accuracy: 0.7832 - loss: 58.3932 - val_accuracy: 0.8366 - val_loss: 54.9235\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - accuracy: 0.8344 - loss: 54.9275\n",
            "Test Loss: 54.9291\n",
            "Test Accuracy: 0.8339\n",
            "Original Sentence: This subject is called quantum natural language.\n",
            "Tokenized Sentence: [[1, 2, 3, 4, 5, 6, 7]]\n",
            "Padded Sentence: [[1 2 3 4 5 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Complex Embedding Magnitude (Amplitude):\n",
            "[[[9.6932799e-03 8.6643882e-03 3.2521565e-03 ... 4.4936812e-04\n",
            "   3.0546548e-04 2.4659955e-03]\n",
            "  [1.9942576e-02 4.1874736e-03 7.3948922e-04 ... 7.5751796e-02\n",
            "   1.0997035e-03 5.7570013e-04]\n",
            "  [3.2301981e-11 6.4095691e-13 9.0914956e-11 ... 1.2387183e-13\n",
            "   1.5033651e-11 1.7684101e-11]\n",
            "  ...\n",
            "  [8.7952204e-03 1.5398126e-02 5.8298919e-02 ... 1.0171119e-03\n",
            "   1.2172137e-02 4.2947852e-03]\n",
            "  [8.7952204e-03 1.5398126e-02 5.8298919e-02 ... 1.0171119e-03\n",
            "   1.2172137e-02 4.2947852e-03]\n",
            "  [8.7952204e-03 1.5398126e-02 5.8298919e-02 ... 1.0171119e-03\n",
            "   1.2172137e-02 4.2947852e-03]]]\n",
            "Global Average Pooled Output:\n",
            "[[0.00875793 0.0205423  0.06567517 0.00039111 0.00490057 0.01577333\n",
            "  0.00242875 0.02484639 0.03555616 0.00296679 0.01528856 0.00796555\n",
            "  0.02113053 0.00057024 0.00242803 0.00120818 0.02081119 0.00261492\n",
            "  0.00938156 0.002665   0.00259351 0.02066782 0.01481306 0.00426814\n",
            "  0.00492374 0.0022597  0.01814846 0.00190852 0.00153399 0.00339828\n",
            "  0.00844157 0.00304188 0.00157856 0.01919928 0.04599    0.00289668\n",
            "  0.00169518 0.02118813 0.00547717 0.00683433 0.03978639 0.04427182\n",
            "  0.07896051 0.00270772 0.01839924 0.00263302 0.02237775 0.00360057\n",
            "  0.00628649 0.05375484 0.01997411 0.00270716 0.00160981 0.01998243\n",
            "  0.00340219 0.05712135 0.00083911 0.00407317 0.03318475 0.07752804\n",
            "  0.00233561 0.03797704 0.05311913 0.02145204 0.01288498 0.01110253\n",
            "  0.00327275 0.00357419 0.0152967  0.00085438 0.08291701 0.01984137\n",
            "  0.00568782 0.04264443 0.0027715  0.01648828 0.01715127 0.00119773\n",
            "  0.02014518 0.0372202  0.01282236 0.00464538 0.00470882 0.02078872\n",
            "  0.0251594  0.00332571 0.0114121  0.00262557 0.0018912  0.01672082\n",
            "  0.01093842 0.00428746 0.01077615 0.06310046 0.0369394  0.02493199\n",
            "  0.01105621 0.00515545 0.02886414 0.00636298]]\n",
            "Final Output (after Dense Layer):\n",
            "[[0.58685017]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4XbD5CTHilxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}