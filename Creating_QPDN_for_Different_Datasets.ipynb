{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_zg7zpgvLgp"
      },
      "source": [
        "## Implementing the QPDN with IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dhct3ejL2ucZ",
        "outputId": "5363c5c6-d2e2-4e83-85e7-cfecb27b53f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_11            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ complex_embedding_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │      \u001b[38;5;34m8,000,000\u001b[0m │ input_layer_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mComplexEmbedding\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ density_matrix_8          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m400\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ complex_embedding_9[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDensityMatrix\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ measurement_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m40,000\u001b[0m │ density_matrix_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mMeasurement\u001b[0m)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ measurement_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m33,792\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_1 (\u001b[38;5;33mAttention\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ bidirectional_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                           │                        │                │ bidirectional_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m164,096\u001b[0m │ attention_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m1,024\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m98,432\u001b[0m │ batch_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_3… │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m66\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_11            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ complex_embedding_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,000,000</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ComplexEmbedding</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ density_matrix_8          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ complex_embedding_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DensityMatrix</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ measurement_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,000</span> │ density_matrix_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Measurement</span>)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ measurement_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,792</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                           │                        │                │ bidirectional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,096</span> │ attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_3… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,348,258\u001b[0m (31.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,348,258</span> (31.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,347,490\u001b[0m (31.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,347,490</span> (31.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 33ms/step - accuracy: 0.5952 - loss: 0.7103 - val_accuracy: 0.8604 - val_loss: 0.3358\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 33ms/step - accuracy: 0.9010 - loss: 0.2532 - val_accuracy: 0.8560 - val_loss: 0.3446\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.9445 - loss: 0.1584 - val_accuracy: 0.8231 - val_loss: 0.4831\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.9589 - loss: 0.1165 - val_accuracy: 0.8036 - val_loss: 0.5606\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.9611 - loss: 0.1124 - val_accuracy: 0.6890 - val_loss: 0.5937\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.8805 - loss: 0.2539 - val_accuracy: 0.6016 - val_loss: 1.4818\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.9580 - loss: 0.1198 - val_accuracy: 0.8131 - val_loss: 0.5604\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 31ms/step - accuracy: 0.9647 - loss: 0.0966 - val_accuracy: 0.6333 - val_loss: 0.5932\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.8033 - loss: 0.3391 - val_accuracy: 0.8359 - val_loss: 0.5384\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9664 - loss: 0.0926 - val_accuracy: 0.6655 - val_loss: 0.6341\n",
            "Epoch 11/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.7957 - loss: 0.3475 - val_accuracy: 0.8386 - val_loss: 0.6253\n",
            "Epoch 12/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.9602 - loss: 0.1071 - val_accuracy: 0.8365 - val_loss: 0.6161\n",
            "Epoch 13/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.9638 - loss: 0.1001 - val_accuracy: 0.8310 - val_loss: 0.6451\n",
            "Epoch 14/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 31ms/step - accuracy: 0.9760 - loss: 0.0733 - val_accuracy: 0.8374 - val_loss: 0.6008\n",
            "Epoch 15/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9852 - loss: 0.0513 - val_accuracy: 0.8276 - val_loss: 0.8501\n",
            "Epoch 16/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.9818 - loss: 0.0580 - val_accuracy: 0.8307 - val_loss: 0.6994\n",
            "Epoch 17/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.9456 - loss: 0.1177 - val_accuracy: 0.8133 - val_loss: 0.6331\n",
            "Epoch 18/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.7729 - loss: 0.3579 - val_accuracy: 0.6411 - val_loss: 0.7859\n",
            "Epoch 19/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.7186 - loss: 0.4036 - val_accuracy: 0.6300 - val_loss: 0.7661\n",
            "Epoch 20/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.7128 - loss: 0.4525 - val_accuracy: 0.6342 - val_loss: 0.8163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.6342\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.92      0.72     12500\n",
            "           1       0.81      0.35      0.49     12500\n",
            "\n",
            "    accuracy                           0.63     25000\n",
            "   macro avg       0.70      0.63      0.60     25000\n",
            "weighted avg       0.70      0.63      0.60     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "Predicted class for the sample text: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class ComplexEmbedding(layers.Layer):\n",
        "    def __init__(self, input_dim, output_dim, **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.real_embedding = layers.Embedding(input_dim, output_dim)\n",
        "        self.imag_embedding = layers.Embedding(input_dim, output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real_part = self.real_embedding(inputs)\n",
        "        imag_part = self.imag_embedding(inputs)\n",
        "        return tf.concat([real_part, imag_part], axis=-1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape + (self.output_dim * 2,)\n",
        "\n",
        "class DensityMatrix(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DensityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real_part, imag_part = tf.split(inputs, 2, axis=-1)\n",
        "        density_matrix_real = tf.einsum('bij,bik->bjk', real_part, real_part) + tf.einsum('bij,bik->bjk', imag_part, imag_part)\n",
        "        density_matrix_imag = tf.einsum('bij,bik->bjk', imag_part, real_part) - tf.einsum('bij,bik->bjk', real_part, imag_part)\n",
        "        return tf.concat([density_matrix_real, density_matrix_imag], axis=-1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1] + (input_shape[-1], input_shape[-1])\n",
        "\n",
        "class Measurement(layers.Layer):\n",
        "    def __init__(self, num_measurements, output_dim, **kwargs):\n",
        "        super(Measurement, self).__init__(**kwargs)\n",
        "        self.num_measurements = num_measurements\n",
        "        self.output_dim = output_dim\n",
        "        self.measurement_vectors_real = self.add_weight(\n",
        "            shape=(num_measurements, output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors_real',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.measurement_vectors_imag = self.add_weight(\n",
        "            shape=(num_measurements, output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors_imag',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        density_matrix_real, density_matrix_imag = tf.split(inputs, 2, axis=-1)\n",
        "        measured_probs_real = tf.einsum('bij,ki->bk', density_matrix_real, self.measurement_vectors_real) - tf.einsum('bij,ki->bk', density_matrix_imag, self.measurement_vectors_imag)\n",
        "        measured_probs_imag = tf.einsum('bij,ki->bk', density_matrix_real, self.measurement_vectors_imag) + tf.einsum('bij,ki->bk', density_matrix_imag, self.measurement_vectors_real)\n",
        "        return tf.sqrt(tf.square(measured_probs_real) + tf.square(measured_probs_imag))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.num_measurements)\n",
        "\n",
        "def create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length):\n",
        "    inputs = layers.Input(shape=(max_length,))\n",
        "    embedding_layer = ComplexEmbedding(vocab_size, embedding_dim)\n",
        "    complex_embeddings = embedding_layer(inputs)\n",
        "    density_matrix_layer = DensityMatrix()\n",
        "    density_matrix = density_matrix_layer(complex_embeddings)\n",
        "    measurement_layer = Measurement(num_measurements, embedding_dim)\n",
        "    measured_probs = measurement_layer(density_matrix)\n",
        "\n",
        "    lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(layers.Reshape((num_measurements, 1))(measured_probs))\n",
        "    attention = layers.Attention()([lstm, lstm])\n",
        "\n",
        "    conv1 = layers.Conv1D(filters=256, kernel_size=5, activation='relu', padding='same')(attention)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv2 = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv1)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    pool = layers.GlobalMaxPooling1D()(conv2)\n",
        "    dropout = layers.Dropout(0.05)(pool)\n",
        "    dense1 = layers.Dense(64, activation='relu')(dropout)\n",
        "    dense1 = layers.Dropout(0.05)(dense1)\n",
        "    dense2 = layers.Dense(32, activation='relu')(dense1)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def load_imdb_data(num_words, maxlen):\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "    return (X_train, y_train), (X_test, y_test), num_words\n",
        "\n",
        "def main():\n",
        "    max_words = 20000\n",
        "    max_length = 200\n",
        "    embedding_dim = 200\n",
        "    num_measurements = 100\n",
        "    num_classes = 2\n",
        "    batch_size = 32\n",
        "    epochs = 20\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test), vocab_size = load_imdb_data(max_words, max_length)\n",
        "\n",
        "    model = create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    model.save(\"qpdn_model.h5\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "    loaded_model = keras.models.load_model(\"qpdn_model.h5\", custom_objects={\n",
        "        'ComplexEmbedding': ComplexEmbedding,\n",
        "        'DensityMatrix': DensityMatrix,\n",
        "        'Measurement': Measurement\n",
        "    })\n",
        "\n",
        "    sample_text = \"This movie was fantastic! The acting was great and the plot was engaging.\"\n",
        "    sample_sequence = imdb.get_word_index()\n",
        "    sample_sequence = [sample_sequence.get(word, 2) for word in sample_text.lower().split()]\n",
        "    sample_sequence = pad_sequences([sample_sequence], maxlen=max_length)\n",
        "    prediction = loaded_model.predict(sample_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    print(f\"Predicted class for the sample text: {predicted_class[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbvxrOFt-nYJ"
      },
      "source": [
        "## Trying some different config for the newsgroup data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTzE9-smbKiR",
        "outputId": "69174f16-5042-4e2c-9e8b-cc6b7fb6db92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample rows of the dataset:\n",
            "Text: sure bashers pen fan pretty confused lack kind post recent pen massacre devil actually bit puzzled bit relieved however going put end non pittsburghers relief bit praise pen man killing devil worse thought jagr showed much better regular season stats also lot fo fun watch playoff bowman let jagr lot fun next couple game since pen going beat pulp jersey anyway disappointed see islander lose final regular season game pen rule...\n",
            "Label: 10\n",
            "\n",
            "Text: brother market high performance video card support vesa local bus 1 2mb ram anyone suggestion idea diamond stealth pro local bus orchid farenheit 1280 ati graphic ultra pro high performance vlb card please post email thank matt...\n",
            "Label: 3\n",
            "\n",
            "Text: finally said dream mediterranean new area greater year like holocaust number july usa sweden april still cold changed calendar nothing mentioned true let say true shall azeri woman child going pay price raped killed tortured armenian hearded something called geneva convention facist ohhh forgot armenian fight nobody forgot killing rapings torture kurd turk upon time ohhhh swedish redcross worker lie ever say regional killer like person shoot policy l confused search turkish plane know talking tu...\n",
            "Label: 17\n",
            "\n",
            "Text: think scsi card dma transfer disk scsi card dma transfer containing data scsi device attached want important feature scsi ability detach device free scsi bus device typically used multi tasking o start transfer several device device seeking data bus free command data transfer device ready transfer data aquire bus send data ide bus start transfer bus busy disk seeked data transfered typically 10 20ms second lock process wanting bus irrespective transfer time...\n",
            "Label: 3\n",
            "\n",
            "Text: 1 old jasmine drive cannot use new system understanding upsate driver modern one order gain compatability system 7 0 1 anyone know inexpensive program seen formatters 20 buit idea work 2 another ancient device one tape drive back utility freeze system try use drive jasmine direct tape bought used 150 w 6 tape techmar mechanism essentially question anyone know inexpensive beckup utility use system 7 0 1...\n",
            "Label: 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import GloVe\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = re.sub(r'\\W+', ' ', text.lower())\n",
        "    tokens = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load and preprocess the 20newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "texts = [preprocess_text(text) for text in newsgroups.data]\n",
        "labels = newsgroups.target\n",
        "\n",
        "# Print some rows of the dataset\n",
        "print(\"Sample rows of the dataset:\")\n",
        "for i in range(5):\n",
        "    print(f\"Text: {texts[i][:500]}...\")  # Print first 500 characters\n",
        "    print(f\"Label: {labels[i]}\")\n",
        "    print()\n",
        "\n",
        "# Create vocabulary\n",
        "word_counts = Counter()\n",
        "for text in texts:\n",
        "    word_counts.update(text.split())\n",
        "\n",
        "vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.items() if count >= 5]\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "embedding_matrix = np.zeros((len(vocab), 100))\n",
        "for word, idx in word2idx.items():\n",
        "    if word in glove.stoi:\n",
        "        embedding_matrix[idx] = glove[word].numpy()\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(100, ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHss488ienEF"
      },
      "outputs": [],
      "source": [
        "# Encode texts\n",
        "max_len = 200\n",
        "encoded_texts = []\n",
        "for text in texts:\n",
        "    encoded = [word2idx.get(word, 1) for word in text.split()[:max_len]]\n",
        "    encoded += [0] * (max_len - len(encoded))\n",
        "    encoded_texts.append(encoded)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(encoded_texts, dtype=torch.long)\n",
        "y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "class NewsGroupsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = NewsGroupsDataset(X_train, y_train)\n",
        "test_dataset = NewsGroupsDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Enhanced Model with Bidirectional LSTM and Attention\n",
        "class EnhancedModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, embedding_matrix):\n",
        "        super(EnhancedModel, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        self.lstm = nn.LSTM(embed_dim, 128, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(256, 1)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embed)\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out).squeeze(-1), dim=-1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        out = self.dropout(attn_out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "num_classes = 20\n",
        "learning_rate = 0.005\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = EnhancedModel(vocab_size, embed_dim, num_classes, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Final evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_x)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OOG_eFI-sUm"
      },
      "source": [
        "## Implementing the QNLP hilbert task for newsgroup data on the same code as the IMDB one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlsPc_dAgu4N",
        "outputId": "a07b74c2-5b62-4393-8f66-64fa8c379331"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " complex_embedding_3 (Compl  (None, 300, 300)             1598040   ['input_4[0][0]']             \n",
            " exEmbedding)                                             0                                       \n",
            "                                                                                                  \n",
            " density_matrix_3 (DensityM  (None, 300, 300)             0         ['complex_embedding_3[0][0]'] \n",
            " atrix)                                                                                           \n",
            "                                                                                                  \n",
            " measurement_3 (Measurement  (None, 150)                  45000     ['density_matrix_3[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLamb  (None, 150, 1)               0         ['measurement_3[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirecti  (None, 150, 512)             528384    ['tf.expand_dims_3[0][0]']    \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " attention_3 (Attention)     (None, 150, 512)             0         ['bidirectional_3[0][0]',     \n",
            "                                                                     'bidirectional_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 150, 512)             1311232   ['attention_3[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 150, 512)             2048      ['conv1d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 150, 256)             393472    ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 150, 256)             1024      ['conv1d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 256)                  0         ['batch_normalization_7[0][0]'\n",
            " obalMaxPooling1D)                                                  ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 256)                  0         ['global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 128)                  32896     ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 128)                  512       ['dense_9[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 128)                  0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 64)                   8256      ['dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 64)                   256       ['dense_10[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 20)                   1300      ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 18304780 (69.83 MB)\n",
            "Trainable params: 18302860 (69.82 MB)\n",
            "Non-trainable params: 1920 (7.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "236/236 [==============================] - 67s 255ms/step - loss: 3.2682 - accuracy: 0.0591 - val_loss: 3.5270 - val_accuracy: 0.0798 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "236/236 [==============================] - 51s 218ms/step - loss: 2.5585 - accuracy: 0.1540 - val_loss: 2.4553 - val_accuracy: 0.1456 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "236/236 [==============================] - 41s 173ms/step - loss: 2.0462 - accuracy: 0.2805 - val_loss: 2.1340 - val_accuracy: 0.2724 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "236/236 [==============================] - 34s 146ms/step - loss: 1.6865 - accuracy: 0.4051 - val_loss: 2.0920 - val_accuracy: 0.3623 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "236/236 [==============================] - 31s 131ms/step - loss: 1.4008 - accuracy: 0.4949 - val_loss: 1.7305 - val_accuracy: 0.4467 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "236/236 [==============================] - 30s 124ms/step - loss: 1.2055 - accuracy: 0.5713 - val_loss: 2.1641 - val_accuracy: 0.4080 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "236/236 [==============================] - 28s 117ms/step - loss: 1.0332 - accuracy: 0.6356 - val_loss: 1.7612 - val_accuracy: 0.5236 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "236/236 [==============================] - 27s 116ms/step - loss: 0.9008 - accuracy: 0.6945 - val_loss: 1.9595 - val_accuracy: 0.5109 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "236/236 [==============================] - 25s 108ms/step - loss: 0.7837 - accuracy: 0.7351 - val_loss: 1.7682 - val_accuracy: 0.5430 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "236/236 [==============================] - 26s 109ms/step - loss: 0.6694 - accuracy: 0.7798 - val_loss: 1.8594 - val_accuracy: 0.5411 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "236/236 [==============================] - 25s 106ms/step - loss: 0.5161 - accuracy: 0.8384 - val_loss: 1.6074 - val_accuracy: 0.6149 - lr: 5.0000e-04\n",
            "Epoch 12/50\n",
            "236/236 [==============================] - 23s 97ms/step - loss: 0.3974 - accuracy: 0.8840 - val_loss: 1.6832 - val_accuracy: 0.6204 - lr: 5.0000e-04\n",
            "Epoch 13/50\n",
            "236/236 [==============================] - 24s 101ms/step - loss: 0.3484 - accuracy: 0.9012 - val_loss: 1.7637 - val_accuracy: 0.6154 - lr: 5.0000e-04\n",
            "Epoch 14/50\n",
            "236/236 [==============================] - 24s 103ms/step - loss: 0.3084 - accuracy: 0.9116 - val_loss: 1.8535 - val_accuracy: 0.6292 - lr: 5.0000e-04\n",
            "Epoch 15/50\n",
            "236/236 [==============================] - 23s 98ms/step - loss: 0.2801 - accuracy: 0.9227 - val_loss: 1.9419 - val_accuracy: 0.6143 - lr: 5.0000e-04\n",
            "Epoch 16/50\n",
            "236/236 [==============================] - 23s 96ms/step - loss: 0.2564 - accuracy: 0.9292 - val_loss: 1.9234 - val_accuracy: 0.6241 - lr: 5.0000e-04\n",
            "Epoch 17/50\n",
            "236/236 [==============================] - 23s 97ms/step - loss: 0.2439 - accuracy: 0.9329 - val_loss: 2.2326 - val_accuracy: 0.6016 - lr: 5.0000e-04\n",
            "Epoch 18/50\n",
            "236/236 [==============================] - 22s 95ms/step - loss: 0.2248 - accuracy: 0.9367 - val_loss: 2.1994 - val_accuracy: 0.6186 - lr: 5.0000e-04\n",
            "Epoch 19/50\n",
            "236/236 [==============================] - 22s 92ms/step - loss: 0.2137 - accuracy: 0.9422 - val_loss: 2.2608 - val_accuracy: 0.5814 - lr: 5.0000e-04\n",
            "Test accuracy: 0.6292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118/118 [==============================] - 2s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.56      0.34       151\n",
            "           1       0.66      0.64      0.65       202\n",
            "           2       0.56      0.56      0.56       195\n",
            "           3       0.53      0.60      0.56       183\n",
            "           4       0.81      0.63      0.71       205\n",
            "           5       0.83      0.71      0.76       215\n",
            "           6       0.68      0.59      0.63       193\n",
            "           7       0.52      0.64      0.57       196\n",
            "           8       0.66      0.64      0.65       168\n",
            "           9       0.88      0.72      0.79       211\n",
            "          10       0.85      0.82      0.83       198\n",
            "          11       0.82      0.67      0.74       201\n",
            "          12       0.60      0.51      0.55       202\n",
            "          13       0.61      0.76      0.68       194\n",
            "          14       0.71      0.67      0.69       189\n",
            "          15       0.74      0.65      0.69       202\n",
            "          16       0.56      0.58      0.57       188\n",
            "          17       0.85      0.73      0.78       182\n",
            "          18       0.59      0.51      0.55       159\n",
            "          19       0.23      0.22      0.23       136\n",
            "\n",
            "    accuracy                           0.63      3770\n",
            "   macro avg       0.65      0.62      0.63      3770\n",
            "weighted avg       0.66      0.63      0.64      3770\n",
            "\n",
            "1/1 [==============================] - 1s 864ms/step\n",
            "Predicted class for the sample text: 7\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, Conv1D, GlobalMaxPooling1D, Dropout, Bidirectional, LSTM, Attention, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class ComplexEmbedding(Layer):\n",
        "    def __init__(self, input_dim, output_dim, **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.real_embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='real_embeddings'\n",
        "        )\n",
        "        self.imag_embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='imag_embeddings'\n",
        "        )\n",
        "        super(ComplexEmbedding, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.int32)\n",
        "        real_part = tf.nn.embedding_lookup(self.real_embeddings, inputs)\n",
        "        imag_part = tf.nn.embedding_lookup(self.imag_embeddings, inputs)\n",
        "        complex_embeddings = tf.complex(real_part, imag_part)\n",
        "        return complex_embeddings\n",
        "\n",
        "class DensityMatrix(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DensityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        weighted_embeddings = inputs\n",
        "        density_matrix = tf.einsum('bij,bik->bjk', weighted_embeddings, tf.math.conj(weighted_embeddings))\n",
        "        return density_matrix\n",
        "\n",
        "class Measurement(Layer):\n",
        "    def __init__(self, num_measurements, output_dim, **kwargs):\n",
        "        super(Measurement, self).__init__(**kwargs)\n",
        "        self.num_measurements = num_measurements\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.measurement_vectors = self.add_weight(\n",
        "            shape=(self.num_measurements, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors'\n",
        "        )\n",
        "        super(Measurement, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        complex_measurement = tf.complex(self.measurement_vectors, tf.zeros_like(self.measurement_vectors))\n",
        "        complex_measurement = tf.linalg.l2_normalize(complex_measurement, axis=-1)\n",
        "        measured_probs = tf.abs(tf.einsum('bij,ki->bk', inputs, complex_measurement))\n",
        "        return measured_probs\n",
        "\n",
        "def create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length):\n",
        "    inputs = tf.keras.Input(shape=(max_length,))\n",
        "    embedding_layer = ComplexEmbedding(vocab_size, embedding_dim)\n",
        "    complex_embeddings = embedding_layer(inputs)\n",
        "    density_matrix_layer = DensityMatrix()\n",
        "    density_matrix = density_matrix_layer(complex_embeddings)\n",
        "    measurement_layer = Measurement(num_measurements, embedding_dim)\n",
        "    measured_probs = measurement_layer(density_matrix)\n",
        "\n",
        "    lstm = Bidirectional(LSTM(256, return_sequences=True))(tf.expand_dims(measured_probs, -1))\n",
        "    attention = Attention()([lstm, lstm])\n",
        "\n",
        "    conv1 = Conv1D(filters=256, kernel_size=10, activation='relu', padding='same')(attention)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool = GlobalMaxPooling1D()(conv2)\n",
        "    dropout = Dropout(0.05)(pool)\n",
        "    dense1 = Dense(128, activation='relu')(dropout)\n",
        "    dense1 = BatchNormalization()(dense1)\n",
        "    dense1 = Dropout(0.05)(dense1)\n",
        "    dense2 = Dense(64, activation='relu')(dense1)\n",
        "    dense2 = BatchNormalization()(dense2)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = re.sub(r'\\W+', ' ', text.lower())\n",
        "    tokens = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_20newsgroups_data(num_words, maxlen):\n",
        "    newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "    texts = [preprocess_text(text) for text in newsgroups.data]\n",
        "    labels = newsgroups.target\n",
        "\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(text.split())\n",
        "\n",
        "    vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.items() if count >= 5]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    encoded_texts = []\n",
        "    for text in texts:\n",
        "        encoded = [word2idx.get(word, 1) for word in text.split()[:maxlen]]\n",
        "        encoded += [0] * (maxlen - len(encoded))\n",
        "        encoded_texts.append(encoded)\n",
        "\n",
        "    X = np.array(encoded_texts)\n",
        "    y = to_categorical(labels, num_classes=len(set(labels)))\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42), len(vocab), word2idx\n",
        "\n",
        "def main():\n",
        "    max_words = 50000\n",
        "    max_length = 300\n",
        "    embedding_dim = 300\n",
        "    num_measurements = 150\n",
        "    num_classes = 20\n",
        "    batch_size = 64\n",
        "    epochs = 50\n",
        "\n",
        "    (X_train, X_test, y_train, y_test), vocab_size, word2idx = load_20newsgroups_data(max_words, max_length)\n",
        "\n",
        "    model = create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def lr_schedule(epoch):\n",
        "        if epoch < 10:\n",
        "            return 0.001\n",
        "        elif epoch < 20:\n",
        "            return 0.0005\n",
        "        else:\n",
        "            return 0.0001\n",
        "\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_scheduler, early_stopping]\n",
        "    )\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    model.save(\"qpdn_model_improved.h5\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "    loaded_model = tf.keras.models.load_model(\"qpdn_model_improved.h5\", custom_objects={\n",
        "        'ComplexEmbedding': ComplexEmbedding,\n",
        "        'DensityMatrix': DensityMatrix,\n",
        "        'Measurement': Measurement\n",
        "    })\n",
        "\n",
        "    sample_text = \"This is a sample document from the 20 newsgroups dataset.\"\n",
        "    sample_sequence = [word2idx.get(word, 1) for word in preprocess_text(sample_text).split()]\n",
        "    sample_sequence = pad_sequences([sample_sequence], maxlen=max_length)\n",
        "    prediction = loaded_model.predict(sample_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    print(f\"Predicted class for the sample text: {predicted_class[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZiLTxMxlrJk"
      },
      "source": [
        "## Implementing the same QPDN approach with reuters dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2BWQpVHkQDr",
        "outputId": "2b3ccf4c-191f-4f67-acef-c12a3b038487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " complex_embedding_3 (Compl  (None, 200, 200)             3200000   ['input_4[0][0]']             \n",
            " exEmbedding)                                             0                                       \n",
            "                                                                                                  \n",
            " density_matrix_3 (DensityM  (None, 200, 200)             0         ['complex_embedding_3[0][0]'] \n",
            " atrix)                                                                                           \n",
            "                                                                                                  \n",
            " measurement_3 (Measurement  (None, 100)                  20000     ['density_matrix_3[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLamb  (None, 100, 1)               0         ['measurement_3[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirecti  (None, 100, 128)             33792     ['tf.expand_dims_3[0][0]']    \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " attention_3 (Attention)     (None, 100, 128)             0         ['bidirectional_3[0][0]',     \n",
            "                                                                     'bidirectional_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 100, 256)             327936    ['attention_3[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 100, 256)             1024      ['conv1d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 100, 128)             98432     ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 100, 128)             512       ['conv1d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 128)                  0         ['batch_normalization_7[0][0]'\n",
            " obalMaxPooling1D)                                                  ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 128)                  0         ['global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 64)                   8256      ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 64)                   0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 32)                   2080      ['dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 46)                   1518      ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 32493550 (123.95 MB)\n",
            "Trainable params: 32492782 (123.95 MB)\n",
            "Non-trainable params: 768 (3.00 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "264/264 [==============================] - 47s 159ms/step - loss: 2.0658 - accuracy: 0.4946 - val_loss: 1.9168 - val_accuracy: 0.5497\n",
            "Epoch 2/20\n",
            "264/264 [==============================] - 27s 101ms/step - loss: 1.5797 - accuracy: 0.5989 - val_loss: 2.0732 - val_accuracy: 0.5607\n",
            "Epoch 3/20\n",
            "264/264 [==============================] - 19s 71ms/step - loss: 1.3370 - accuracy: 0.6483 - val_loss: 1.5505 - val_accuracy: 0.6120\n",
            "Epoch 4/20\n",
            "264/264 [==============================] - 13s 51ms/step - loss: 1.0902 - accuracy: 0.7277 - val_loss: 1.3655 - val_accuracy: 0.6744\n",
            "Epoch 5/20\n",
            "264/264 [==============================] - 11s 41ms/step - loss: 0.9175 - accuracy: 0.7670 - val_loss: 1.3062 - val_accuracy: 0.6844\n",
            "Epoch 6/20\n",
            "264/264 [==============================] - 10s 37ms/step - loss: 0.7570 - accuracy: 0.8028 - val_loss: 1.3104 - val_accuracy: 0.7015\n",
            "Epoch 7/20\n",
            "264/264 [==============================] - 10s 39ms/step - loss: 0.6577 - accuracy: 0.8265 - val_loss: 1.3590 - val_accuracy: 0.6929\n",
            "Epoch 8/20\n",
            "264/264 [==============================] - 8s 30ms/step - loss: 0.5613 - accuracy: 0.8539 - val_loss: 1.3419 - val_accuracy: 0.7132\n",
            "Epoch 9/20\n",
            "264/264 [==============================] - 9s 34ms/step - loss: 0.4950 - accuracy: 0.8732 - val_loss: 1.4882 - val_accuracy: 0.7114\n",
            "Epoch 10/20\n",
            "264/264 [==============================] - 8s 31ms/step - loss: 0.4209 - accuracy: 0.8951 - val_loss: 1.3514 - val_accuracy: 0.7382\n",
            "Epoch 11/20\n",
            "264/264 [==============================] - 8s 29ms/step - loss: 0.3777 - accuracy: 0.9050 - val_loss: 1.4025 - val_accuracy: 0.7339\n",
            "Epoch 12/20\n",
            "264/264 [==============================] - 8s 29ms/step - loss: 0.3263 - accuracy: 0.9141 - val_loss: 1.5496 - val_accuracy: 0.7100\n",
            "Epoch 13/20\n",
            "264/264 [==============================] - 7s 28ms/step - loss: 0.2991 - accuracy: 0.9241 - val_loss: 1.4604 - val_accuracy: 0.7235\n",
            "Epoch 14/20\n",
            "264/264 [==============================] - 7s 28ms/step - loss: 0.2721 - accuracy: 0.9325 - val_loss: 1.4297 - val_accuracy: 0.7289\n",
            "Epoch 15/20\n",
            "264/264 [==============================] - 8s 30ms/step - loss: 0.2508 - accuracy: 0.9341 - val_loss: 1.9855 - val_accuracy: 0.6331\n",
            "Epoch 16/20\n",
            "264/264 [==============================] - 8s 29ms/step - loss: 0.2403 - accuracy: 0.9333 - val_loss: 1.6082 - val_accuracy: 0.7282\n",
            "Epoch 17/20\n",
            "264/264 [==============================] - 8s 29ms/step - loss: 0.2464 - accuracy: 0.9317 - val_loss: 1.5395 - val_accuracy: 0.7278\n",
            "Epoch 18/20\n",
            "264/264 [==============================] - 7s 27ms/step - loss: 0.2117 - accuracy: 0.9412 - val_loss: 1.5909 - val_accuracy: 0.7417\n",
            "Epoch 19/20\n",
            "264/264 [==============================] - 8s 29ms/step - loss: 0.1941 - accuracy: 0.9466 - val_loss: 1.8011 - val_accuracy: 0.7211\n",
            "Epoch 20/20\n",
            "264/264 [==============================] - 7s 25ms/step - loss: 0.1938 - accuracy: 0.9447 - val_loss: 1.5700 - val_accuracy: 0.7460\n",
            "Test accuracy: 0.7460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "88/88 [==============================] - 1s 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.57      0.64        14\n",
            "           1       0.76      0.74      0.75       134\n",
            "           2       0.44      0.70      0.54        27\n",
            "           3       0.92      0.90      0.91       994\n",
            "           4       0.79      0.84      0.81       600\n",
            "           5       0.22      0.29      0.25         7\n",
            "           6       0.88      0.82      0.85        17\n",
            "           7       0.17      0.25      0.20         4\n",
            "           8       0.50      0.71      0.59        48\n",
            "           9       0.71      0.81      0.76        31\n",
            "          10       0.75      0.79      0.77        38\n",
            "          11       0.65      0.61      0.63       109\n",
            "          12       0.36      0.22      0.28        18\n",
            "          13       0.49      0.50      0.49        46\n",
            "          14       0.67      0.40      0.50         5\n",
            "          15       0.15      0.22      0.18         9\n",
            "          16       0.70      0.61      0.65       128\n",
            "          17       0.57      0.29      0.38        14\n",
            "          18       0.50      0.40      0.44        25\n",
            "          19       0.64      0.66      0.65       174\n",
            "          20       0.51      0.38      0.43        80\n",
            "          21       0.67      0.45      0.54        31\n",
            "          22       0.33      0.09      0.14        11\n",
            "          23       0.60      0.60      0.60        15\n",
            "          24       0.43      0.39      0.41        23\n",
            "          25       0.67      0.74      0.70        38\n",
            "          26       0.60      0.50      0.55        12\n",
            "          27       0.27      0.60      0.37         5\n",
            "          28       0.33      0.25      0.29        12\n",
            "          29       0.11      0.25      0.15         4\n",
            "          30       0.80      0.25      0.38        16\n",
            "          31       0.46      0.38      0.41        16\n",
            "          32       0.46      0.55      0.50        11\n",
            "          33       0.67      0.86      0.75         7\n",
            "          34       0.62      0.45      0.53        11\n",
            "          35       0.33      0.17      0.22         6\n",
            "          36       0.28      0.42      0.33        12\n",
            "          37       0.29      1.00      0.44         2\n",
            "          38       0.40      0.80      0.53         5\n",
            "          39       0.17      0.14      0.15         7\n",
            "          40       0.22      0.31      0.26        13\n",
            "          41       0.50      0.38      0.43         8\n",
            "          42       0.40      0.50      0.44         4\n",
            "          43       0.42      0.71      0.53         7\n",
            "          44       0.75      0.50      0.60         6\n",
            "          45       0.20      0.67      0.31         3\n",
            "\n",
            "    accuracy                           0.75      2807\n",
            "   macro avg       0.50      0.51      0.48      2807\n",
            "weighted avg       0.75      0.75      0.75      2807\n",
            "\n",
            "1/1 [==============================] - 1s 728ms/step\n",
            "Predicted class for the sample text: 4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, Conv1D, GlobalMaxPooling1D, Dropout, Bidirectional, LSTM, Attention, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define a custom layer for complex embeddings\n",
        "class ComplexEmbedding(Layer):\n",
        "    def __init__(self, input_dim, output_dim, **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.real_embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='real_embeddings'\n",
        "        )\n",
        "        self.imag_embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='imag_embeddings'\n",
        "        )\n",
        "        super(ComplexEmbedding, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.int32)\n",
        "        real_part = tf.nn.embedding_lookup(self.real_embeddings, inputs)\n",
        "        imag_part = tf.nn.embedding_lookup(self.imag_embeddings, inputs)\n",
        "        complex_embeddings = tf.complex(real_part, imag_part)\n",
        "        return complex_embeddings\n",
        "\n",
        "# Define a custom layer for creating density matrices from embeddings\n",
        "class DensityMatrix(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DensityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        weighted_embeddings = inputs\n",
        "        density_matrix = tf.einsum('bij,bik->bjk', weighted_embeddings, tf.math.conj(weighted_embeddings))\n",
        "        return density_matrix\n",
        "\n",
        "# Define a custom layer for performing measurement on the density matrix\n",
        "class Measurement(Layer):\n",
        "    def __init__(self, num_measurements, output_dim, **kwargs):\n",
        "        super(Measurement, self).__init__(**kwargs)\n",
        "        self.num_measurements = num_measurements\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.measurement_vectors = self.add_weight(\n",
        "            shape=(self.num_measurements, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors'\n",
        "        )\n",
        "        super(Measurement, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        complex_measurement = tf.complex(self.measurement_vectors, tf.zeros_like(self.measurement_vectors))\n",
        "        complex_measurement = tf.linalg.l2_normalize(complex_measurement, axis=-1)\n",
        "        measured_probs = tf.abs(tf.einsum('bij,ki->bk', inputs, complex_measurement))\n",
        "        return measured_probs\n",
        "\n",
        "# Define the QPDN model\n",
        "def create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length):\n",
        "    inputs = tf.keras.Input(shape=(max_length,))\n",
        "    embedding_layer = ComplexEmbedding(vocab_size, embedding_dim)\n",
        "    complex_embeddings = embedding_layer(inputs)\n",
        "    density_matrix_layer = DensityMatrix()\n",
        "    density_matrix = density_matrix_layer(complex_embeddings)\n",
        "    measurement_layer = Measurement(num_measurements, embedding_dim)\n",
        "    measured_probs = measurement_layer(density_matrix)\n",
        "\n",
        "    lstm = Bidirectional(LSTM(64, return_sequences=True))(tf.expand_dims(measured_probs, -1))\n",
        "    attention = Attention()([lstm, lstm])\n",
        "\n",
        "    conv1 = Conv1D(filters=256, kernel_size=10, activation='relu', padding='same')(attention)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool = GlobalMaxPooling1D()(conv2)\n",
        "    dropout = Dropout(0.05)(pool)\n",
        "    dense1 = Dense(64, activation='relu')(dropout)\n",
        "    dense1 = Dropout(0.05)(dense1)\n",
        "    dense2 = Dense(32, activation='relu')(dense1)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Load and preprocess Reuters data\n",
        "def load_reuters_data(num_words, maxlen):\n",
        "    (X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.25)\n",
        "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "    num_classes = len(set(y_train))  # Number of classes in the dataset\n",
        "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "    return (X_train, y_train), (X_test, y_test), num_words, num_classes\n",
        "\n",
        "def main():\n",
        "    max_words = 80000\n",
        "    max_length = 200\n",
        "    embedding_dim = 200\n",
        "    num_measurements = 100\n",
        "    batch_size = 32\n",
        "    epochs = 20\n",
        "\n",
        "    # Load the Reuters dataset\n",
        "    (X_train, y_train), (X_test, y_test), vocab_size, num_classes = load_reuters_data(max_words, max_length)\n",
        "\n",
        "    # Create the QPDN model\n",
        "    model = create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Display the model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save(\"qpdn_model_reuters.h5\")\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "    # Load the saved model for testing with a sample\n",
        "    loaded_model = tf.keras.models.load_model(\"qpdn_model_reuters.h5\", custom_objects={\n",
        "        'ComplexEmbedding': ComplexEmbedding,\n",
        "        'DensityMatrix': DensityMatrix,\n",
        "        'Measurement': Measurement\n",
        "    })\n",
        "\n",
        "    # Test with a sample text\n",
        "    sample_text = \"This is a sample Reuters news article text.\"\n",
        "    sample_sequence = reuters.get_word_index()\n",
        "    sample_sequence = [sample_sequence.get(word, 2) for word in sample_text.lower().split()]\n",
        "    sample_sequence = pad_sequences([sample_sequence], maxlen=max_length)\n",
        "    prediction = loaded_model.predict(sample_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    print(f\"Predicted class for the sample text: {predicted_class[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvement for 20 Newsgroup Dataset - QPDN Approach"
      ],
      "metadata": {
        "id": "EMQ97phL9YRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdYsK89Dp8Ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361c73d7-ab09-429b-ac43-817a3fd6e78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n",
            "\n",
            "```\n",
            "x = Input(...)\n",
            "...\n",
            "tf_fn(x)  # Invalid.\n",
            "```\n",
            "\n",
            "What you should do instead is wrap `tf_fn` in a layer:\n",
            "\n",
            "```\n",
            "class MyLayer(Layer):\n",
            "    def call(self, x):\n",
            "        return tf_fn(x)\n",
            "\n",
            "x = MyLayer()(x)\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, Conv1D, GlobalMaxPooling1D, Dropout, Bidirectional, LSTM, Attention, BatchNormalization, Embedding, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class ComplexEmbedding(Layer):\n",
        "    def __init__(self, input_dim, output_dim, **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.real_embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='real_embeddings'\n",
        "        )\n",
        "        self.imag_embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='imag_embeddings'\n",
        "        )\n",
        "        super(ComplexEmbedding, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.int32)\n",
        "        real_part = tf.nn.embedding_lookup(self.real_embeddings, inputs)\n",
        "        imag_part = tf.nn.embedding_lookup(self.imag_embeddings, inputs)\n",
        "        complex_embeddings = tf.complex(real_part, imag_part)\n",
        "        return complex_embeddings\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], self.output_dim)\n",
        "\n",
        "class DensityMatrix(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DensityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        weighted_embeddings = inputs\n",
        "        density_matrix = tf.einsum('bij,bik->bjk', weighted_embeddings, tf.math.conj(weighted_embeddings))\n",
        "        print(\"Density Matrix dtype:\", density_matrix.dtype)  # Print dtype of density matrix\n",
        "        return density_matrix\n",
        "\n",
        "class Measurement(Layer):\n",
        "    def __init__(self, num_measurements, output_dim, **kwargs):\n",
        "        super(Measurement, self).__init__(**kwargs)\n",
        "        self.num_measurements = num_measurements\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.measurement_vectors = self.add_weight(\n",
        "            shape=(self.num_measurements, self.output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors'\n",
        "        )\n",
        "        super(Measurement, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        print(\"Measurement Input dtype:\", inputs.dtype)  # Print dtype of input to Measurement\n",
        "        complex_measurement = tf.complex(self.measurement_vectors, tf.zeros_like(self.measurement_vectors))\n",
        "        complex_measurement = tf.linalg.l2_normalize(complex_measurement, axis=-1)\n",
        "        measured_probs = tf.abs(tf.einsum('bij,ki->bk', tf.cast(inputs, tf.complex64), complex_measurement))\n",
        "        return measured_probs\n",
        "\n",
        "def create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length):\n",
        "    inputs = tf.keras.Input(shape=(max_length,))\n",
        "    embedding_layer = ComplexEmbedding(vocab_size, embedding_dim)\n",
        "    complex_embeddings = embedding_layer(inputs)\n",
        "    density_matrix_layer = DensityMatrix()\n",
        "    density_matrix = density_matrix_layer(complex_embeddings)\n",
        "    measurement_layer = Measurement(num_measurements, embedding_dim)\n",
        "    measured_probs = measurement_layer(density_matrix)\n",
        "\n",
        "    # Wrap tf.expand_dims in a Lambda layer\n",
        "    lstm_input = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, -1))(measured_probs)\n",
        "    lstm = Bidirectional(LSTM(256, return_sequences=True))(lstm_input)\n",
        "    attention = Attention()([lstm, lstm])\n",
        "\n",
        "    conv1 = Conv1D(filters=256, kernel_size=10, activation='relu', padding='same', kernel_regularizer=l2(0.01))(attention)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(conv1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool = GlobalMaxPooling1D()(conv2)\n",
        "    dropout = Dropout(0.1)(pool)\n",
        "    dense1 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(dropout)\n",
        "    dense1 = BatchNormalization()(dense1)\n",
        "    dense1 = Dropout(0.1)(dense1)\n",
        "    dense2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(dense1)\n",
        "    dense2 = BatchNormalization()(dense2)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = re.sub(r'\\W+', ' ', text.lower())\n",
        "    tokens = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_20newsgroups_data(num_words, maxlen):\n",
        "    newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "    texts = [preprocess_text(text) for text in newsgroups.data]\n",
        "    labels = newsgroups.target\n",
        "\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(text.split())\n",
        "\n",
        "    vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.items() if count >= 5]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    encoded_texts = []\n",
        "    for text in texts:\n",
        "        encoded = [word2idx.get(word, 1) for word in text.split()[:maxlen]]\n",
        "        encoded += [0] * (maxlen - len(encoded))\n",
        "        encoded_texts.append(encoded)\n",
        "\n",
        "    X = np.array(encoded_texts)\n",
        "    y = to_categorical(labels, num_classes=len(set(labels)))\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42), len(vocab), word2idx\n",
        "\n",
        "def main():\n",
        "    max_words = 50000\n",
        "    max_length = 300\n",
        "    embedding_dim = 300\n",
        "    num_measurements = 150\n",
        "    num_classes = 20\n",
        "    batch_size = 64\n",
        "    epochs = 50\n",
        "\n",
        "    (X_train, X_test, y_train, y_test), vocab_size, word2idx = load_20newsgroups_data(max_words, max_length)\n",
        "\n",
        "    model = create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def lr_schedule(epoch):\n",
        "        if epoch < 10:\n",
        "            return 0.001\n",
        "        elif epoch < 20:\n",
        "            return 0.0005\n",
        "        else:\n",
        "            return 0.0001\n",
        "\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_scheduler, early_stopping]\n",
        "    )\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    model.save(\"qpdn_model_improved.h5\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "    loaded_model = tf.keras.models.load_model(\"qpdn_model_improved.h5\", custom_objects={\n",
        "        'ComplexEmbedding': ComplexEmbedding,\n",
        "        'DensityMatrix': DensityMatrix,\n",
        "        'Measurement': Measurement\n",
        "    })\n",
        "\n",
        "    sample_text = \"This is a sample document from the 20 newsgroups dataset.\"\n",
        "    sample_sequence = [word2idx.get(word, 1) for word in preprocess_text(sample_text).split()]\n",
        "    sample_sequence = pad_sequences([sample_sequence], maxlen=max_length)\n",
        "    prediction = loaded_model.predict(sample_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    print(f\"Predicted class for the sample text: {predicted_class[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "\n",
        "class ComplexEmbedding(layers.Layer):\n",
        "    def __init__(self, input_dim, output_dim, **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.real_embedding = layers.Embedding(input_dim, output_dim)\n",
        "        self.imag_embedding = layers.Embedding(input_dim, output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real_part = self.real_embedding(inputs)\n",
        "        imag_part = self.imag_embedding(inputs)\n",
        "        return tf.concat([real_part, imag_part], axis=-1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape + (self.output_dim * 2,)\n",
        "\n",
        "class DensityMatrix(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DensityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real_part, imag_part = tf.split(inputs, 2, axis=-1)\n",
        "        density_matrix_real = tf.einsum('bij,bik->bjk', real_part, real_part) + tf.einsum('bij,bik->bjk', imag_part, imag_part)\n",
        "        density_matrix_imag = tf.einsum('bij,bik->bjk', imag_part, real_part) - tf.einsum('bij,bik->bjk', real_part, imag_part)\n",
        "        return tf.concat([density_matrix_real, density_matrix_imag], axis=-1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1] + (input_shape[-1], input_shape[-1])\n",
        "\n",
        "class Measurement(layers.Layer):\n",
        "    def __init__(self, num_measurements, output_dim, **kwargs):\n",
        "        super(Measurement, self).__init__(**kwargs)\n",
        "        self.num_measurements = num_measurements\n",
        "        self.output_dim = output_dim\n",
        "        self.measurement_vectors_real = self.add_weight(\n",
        "            shape=(num_measurements, output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors_real',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.measurement_vectors_imag = self.add_weight(\n",
        "            shape=(num_measurements, output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors_imag',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        density_matrix_real, density_matrix_imag = tf.split(inputs, 2, axis=-1)\n",
        "        measured_probs_real = tf.einsum('bij,ki->bk', density_matrix_real, self.measurement_vectors_real) - tf.einsum('bij,ki->bk', density_matrix_imag, self.measurement_vectors_imag)\n",
        "        measured_probs_imag = tf.einsum('bij,ki->bk', density_matrix_real, self.measurement_vectors_imag) + tf.einsum('bij,ki->bk', density_matrix_imag, self.measurement_vectors_real)\n",
        "        return tf.sqrt(tf.square(measured_probs_real) + tf.square(measured_probs_imag))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.num_measurements)\n",
        "\n",
        "def create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length):\n",
        "    inputs = layers.Input(shape=(max_length,))\n",
        "    embedding_layer = ComplexEmbedding(vocab_size, embedding_dim)\n",
        "    complex_embeddings = embedding_layer(inputs)\n",
        "    density_matrix_layer = DensityMatrix()\n",
        "    density_matrix = density_matrix_layer(complex_embeddings)\n",
        "    measurement_layer = Measurement(num_measurements, embedding_dim)\n",
        "    measured_probs = measurement_layer(density_matrix)\n",
        "\n",
        "    lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(layers.Reshape((num_measurements, 1))(measured_probs))\n",
        "    attention = layers.Attention()([lstm, lstm])\n",
        "\n",
        "    conv1 = layers.Conv1D(filters=256, kernel_size=5, activation='relu', padding='same')(attention)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv2 = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv1)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    pool = layers.GlobalMaxPooling1D()(conv2)\n",
        "    dropout = layers.Dropout(0.05)(pool)\n",
        "    dense1 = layers.Dense(64, activation='relu')(dropout)\n",
        "    dense1 = layers.Dropout(0.05)(dense1)\n",
        "    dense2 = layers.Dense(32, activation='relu')(dense1)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Save the complex embeddings using a Keras model\n",
        "    embedding_extraction_model = Model(inputs=inputs, outputs=complex_embeddings)\n",
        "    return model, embedding_extraction_model\n",
        "\n",
        "def load_reuters_data(num_words, maxlen):\n",
        "    (X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.25)\n",
        "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "    num_classes = np.max(y_train) + 1\n",
        "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "    return (X_train, y_train), (X_test, y_test), num_words, num_classes\n",
        "\n",
        "def main():\n",
        "    max_words = 20000\n",
        "    max_length = 200\n",
        "    embedding_dim = 200\n",
        "    num_measurements = 100\n",
        "    batch_size = 32\n",
        "    epochs = 20\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test), vocab_size, num_classes = load_reuters_data(max_words, max_length)\n",
        "\n",
        "    model, embedding_extraction_model = create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    model.save(\"qpdn_model.h5\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "    loaded_model = keras.models.load_model(\"qpdn_model.h5\", custom_objects={\n",
        "        'ComplexEmbedding': ComplexEmbedding,\n",
        "        'DensityMatrix': DensityMatrix,\n",
        "        'Measurement': Measurement\n",
        "    })\n",
        "\n",
        "    # Save complex embeddings for a sample batch\n",
        "    complex_embeddings = embedding_extraction_model.predict(X_test[:batch_size])\n",
        "    with open(\"complex_embeddings.pkl\", \"wb\") as f:\n",
        "        pickle.dump(complex_embeddings, f)\n",
        "\n",
        "    sample_text = \"This is a sample Reuters news article text.\"\n",
        "    sample_sequence = reuters.get_word_index()\n",
        "    sample_sequence = [sample_sequence.get(word, 2) for word in sample_text.lower().split()]\n",
        "    sample_sequence = pad_sequences([sample_sequence], maxlen=max_length)\n",
        "    prediction = loaded_model.predict(sample_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    print(f\"Predicted class for the sample text: {predicted_class[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iWeZpO29olA9",
        "outputId": "9ae58d71-5ecd-48a1-9f26-c7cb7a0cfbe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ complex_embedding_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │      \u001b[38;5;34m8,000,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mComplexEmbedding\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ density_matrix_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m400\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ complex_embedding_1[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDensityMatrix\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ measurement_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m40,000\u001b[0m │ density_matrix_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mMeasurement\u001b[0m)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ measurement_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m33,792\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (\u001b[38;5;33mAttention\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m164,096\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m1,024\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m98,432\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)             │          \u001b[38;5;34m1,518\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ complex_embedding_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,000,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ComplexEmbedding</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ density_matrix_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ complex_embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DensityMatrix</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ measurement_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,000</span> │ density_matrix_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Measurement</span>)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ measurement_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,792</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,096</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,518</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,349,710\u001b[0m (31.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,349,710</span> (31.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,348,942\u001b[0m (31.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,348,942</span> (31.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m256/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 562ms/step - accuracy: 0.3134 - loss: 2.7066"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7734755dcdaa>\u001b[0m in \u001b[0;36m<cell line: 159>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-7734755dcdaa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import pickle\n",
        "\n",
        "class ComplexEmbedding(layers.Layer):\n",
        "    def __init__(self, input_dim, output_dim, **kwargs):\n",
        "        super(ComplexEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.real_embedding = layers.Embedding(input_dim, output_dim)\n",
        "        self.imag_embedding = layers.Embedding(input_dim, output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real_part = self.real_embedding(inputs)\n",
        "        imag_part = self.imag_embedding(inputs)\n",
        "        return tf.concat([real_part, imag_part], axis=-1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape + (self.output_dim * 2,)\n",
        "\n",
        "class DensityMatrix(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DensityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        real_part, imag_part = tf.split(inputs, 2, axis=-1)\n",
        "        density_matrix_real = tf.einsum('bij,bik->bjk', real_part, real_part) + tf.einsum('bij,bik->bjk', imag_part, imag_part)\n",
        "        density_matrix_imag = tf.einsum('bij,bik->bjk', imag_part, real_part) - tf.einsum('bij,bik->bjk', real_part, imag_part)\n",
        "        return tf.concat([density_matrix_real, density_matrix_imag], axis=-1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1] + (input_shape[-1], input_shape[-1])\n",
        "\n",
        "class Measurement(layers.Layer):\n",
        "    def __init__(self, num_measurements, output_dim, **kwargs):\n",
        "        super(Measurement, self).__init__(**kwargs)\n",
        "        self.num_measurements = num_measurements\n",
        "        self.output_dim = output_dim\n",
        "        self.measurement_vectors_real = self.add_weight(\n",
        "            shape=(num_measurements, output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors_real',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.measurement_vectors_imag = self.add_weight(\n",
        "            shape=(num_measurements, output_dim),\n",
        "            initializer='glorot_uniform',\n",
        "            name='measurement_vectors_imag',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        density_matrix_real, density_matrix_imag = tf.split(inputs, 2, axis=-1)\n",
        "        measured_probs_real = tf.einsum('bij,ki->bk', density_matrix_real, self.measurement_vectors_real) - tf.einsum('bij,ki->bk', density_matrix_imag, self.measurement_vectors_imag)\n",
        "        measured_probs_imag = tf.einsum('bij,ki->bk', density_matrix_real, self.measurement_vectors_imag) + tf.einsum('bij,ki->bk', density_matrix_imag, self.measurement_vectors_real)\n",
        "        return tf.sqrt(tf.square(measured_probs_real) + tf.square(measured_probs_imag))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.num_measurements)\n",
        "\n",
        "def create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length):\n",
        "    inputs = layers.Input(shape=(max_length,))\n",
        "    embedding_layer = ComplexEmbedding(vocab_size, embedding_dim)\n",
        "    complex_embeddings = embedding_layer(inputs)\n",
        "    density_matrix_layer = DensityMatrix()\n",
        "    density_matrix = density_matrix_layer(complex_embeddings)\n",
        "    measurement_layer = Measurement(num_measurements, embedding_dim)\n",
        "    measured_probs = measurement_layer(density_matrix)\n",
        "\n",
        "    lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(layers.Reshape((num_measurements, 1))(measured_probs))\n",
        "    attention = layers.Attention()([lstm, lstm])\n",
        "\n",
        "    conv1 = layers.Conv1D(filters=256, kernel_size=5, activation='relu', padding='same')(attention)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv2 = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv1)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    pool = layers.GlobalMaxPooling1D()(conv2)\n",
        "    dropout = layers.Dropout(0.05)(pool)\n",
        "    dense1 = layers.Dense(64, activation='relu')(dropout)\n",
        "    dense1 = layers.Dropout(0.05)(dense1)\n",
        "    dense2 = layers.Dense(32, activation='relu')(dense1)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Save the complex embeddings using a Keras model\n",
        "    embedding_extraction_model = Model(inputs=inputs, outputs=complex_embeddings)\n",
        "    return model, embedding_extraction_model\n",
        "\n",
        "def load_reuters_data(num_words, maxlen):\n",
        "    (X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.25)\n",
        "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "    num_classes = np.max(y_train) + 1\n",
        "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "    return (X_train, y_train), (X_test, y_test), num_words, num_classes\n",
        "\n",
        "def main():\n",
        "    max_words = 20000\n",
        "    max_length = 200\n",
        "    embedding_dim = 200\n",
        "    num_measurements = 100\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test), vocab_size, num_classes = load_reuters_data(max_words, max_length)\n",
        "\n",
        "    _, embedding_extraction_model = create_qpdn_model(vocab_size, embedding_dim, num_measurements, num_classes, max_length)\n",
        "\n",
        "    # Save complex embeddings for a sample batch\n",
        "    complex_embeddings = embedding_extraction_model.predict(X_train[:32])  # Using a batch size of 32 for example\n",
        "    with open(\"complex_embeddings.pkl\", \"wb\") as f:\n",
        "        pickle.dump(complex_embeddings, f)\n",
        "\n",
        "    print(\"Complex embeddings saved to complex_embeddings.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ee1bK7iZnXDh",
        "outputId": "f2d7f1e4-5406-4309-8f68-2d3238dc64e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "Complex embeddings saved to complex_embeddings.pkl\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}